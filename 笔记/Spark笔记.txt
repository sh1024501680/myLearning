一、Spark概述
    (1)Spark 是专为大规模数据处理而设计的快速通用的计算引擎。
    
    (2) Spark，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的
    是——Job中间输出结果可以  保存在内存中，从而不再需要读写HDFS，
    因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。
    
    (3)Spark 是在 Scala 语言中实现的
    
    (4)Spark 主要有三个特点：
        1.首先，高级 API 剥离了对集群本身的关注，Spark 应用开发者可以专注于应用所要做的计算本身。
        2.其次，Spark 很快，支持交互式计算和复杂算法。
        3.最后，Spark 是一个通用引擎，可用它来完成各种各样的运算，包括 SQL 查询、文本处理、机器
        学习等，而在 Spark 出现之前，我们一般需要学习各种各样的引擎来分别处理这些需求。
    (5)Spark 的内置项目：
        Spark Core:实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储
                   系统交互等模块。Spark Core中还包含了对于弹性分布式数据及RDD的API定义。
        Spark SQL:是Spark用来操作结构化数据的工具包。通过Spark SQL，我们可以使用SQL或者ApacheHive
                  版本的SQL方言来查询数据。Spark SQL支持多种数据源，如Hive表、Parquet以及JSON等。
        Spark Streaming:是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，
                        并且与Spark Core中的RDD API高度对应。
        Spark MLlib:提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了
                    模型评估、数据导入等额外的支持功能。
        集群管理器:Spark设计为可以高效地在一个和计算节点到数千个计算节点之间伸缩计算。为了实现这样
                   的要求，同时获得最大灵活性，Spark支持在各种集群管理器(cluster manager)上运行，
                   包括(Hadoop YARN)、Apache Mesos，以及Spark自带的一个简易调度器，叫做独立调度器。
二、Spark的安装
    安装前准备：
        Linux虚拟机3台(需要Java环境)、Spark安装包(适配Hadoop版本)
    (1)解压安装包到指定位置：
        tar -xf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/install/
    (2)配置Spark(standalone)：进入Spark目录下(/opt/install/spark-2.1.1-bin-hadoop2.7)
        $ cp slaves.template slaves
        $ vim slaves (添加作为work节点的主机名或IP地址)
        $ cp spark-env.sh.template spark-env.sh
        $ vim spark-env.sh  
            1.添加如下配置：
                SPARK_MASTER_HOST=Hadoop01   (含义为作为Spark的master节点的机器)
                SPARK_MASTER_PORT=7077        (Spark默认端口)
            2.高可用配置(需先启动zookeeper)：
                *删除单master时配置的SPARK_MASTER_HOST属性
                添加如下配置：
                export SPARK_DAEMON_JAVA_OPTS="
                            -Dspark.deploy.recoveryMode=ZOOKEEPER
                            -Dspark.deploy.zookeeper.url=hadoop01,hadoop02,hadoop03
                            -Dspark.deploy.zookeeper.dir=/spark"
            3.配置yarn模式
                $ vim spark-default.conf
                添加：
                spark.yarn.historyServer.address=hadoop-senior01:18080
                spark.history.ui.port=18080
    (3)配置Job History Server
        $ cp spark-default.conf.template spark-default.conf
        $ vim spark-default.conf 添加如下内容
            spark.master        spark://hadoop01:7077
            spark.eventLog.enabled  true
            spark.eventLog.dir    hdfs://mycluster/directory  
            (小提示，hadoop高可用集群使用集群别名作为hdfs的URL时需要将hadoop的core-site.xml
            以及hdfs-site.xml复制在当前配置文件目录下或以软链接的形式在当前配置文件目录创建)
        $ vim spark-env.sh
            export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=4000
                                       -Dspark.history.retainedApplications=3
                                       -Dspark.history.fs.logDirectory=hdfs://mycluster/directory"
            参数介绍：spark.history.ui.port=4000 --> 调整WEBUI访问端口号为4000
            spark.history.retainedApplications=3 --> 指定保存Application的历史记录的个数，如果
                                    超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数
            spark.history.fs.logDirectory --> 配置了改属性，start-history-server.sh无需显示指定
                                    路径，SPARK History Server页面只展示该路径下的信息
    (4)同步配置文件到集群所有节点            
三、Spark相关概念
    1.Driver
        Spark驱动器节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责：
        > 将用户程序转化为作业(Job)
        > 在Executor之间调度任务(task)
        > 跟踪Executor的执行情况
        > 通过UI展示查询运行情况
    2.executor
            Spark Executor是集群中工作节点(Worker)中的一个JVM进程，负责在Spark作业中运行具体任务(Task)，任务彼此之间相互独立。
        Spark应用启动时，Executor节点被同时启动，并且始终伴随着整个Spark应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，
        Spark应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。
        Executor两个核心功能：
        > 负责运行组成Spark应用的任务，并将结果返回给驱动器进程
        > 它们通过自身的块管理器(Block Manager)为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在Executor进程内的，因此任务
        可以在运行时充分利用缓存数据加速运算。
    3.Master & Worker
        Master：负责资源调度和分配的一个进程，并进行集群的监控等职责，类似于Yarn中的RM
        Worker：一个Worker运行在集群中的一台服务器上，由Mater分配资源对数据进行并行的处理和计算，类似于Yarn中的NM
    4.并行度(Parallelism)
        整个集群并行执行任务的数量称之为并行度。
    5.SparkContext：
            SparkConf(配置对象)：基础环境配置
            SparkEnv(环境对象):通信环境
            SchedulerBackend(通信后台):主要用于和Executor之间进行通信
            TaskScheduler(任务调度器):主要用于任务的调度
            DAGScheduler(阶段调度器):主要用于阶段的划分及任务的切分
            
四、Spark Core
    Spark Core的核心工作：操作RDD
        RDD的创建->转换->缓存->行动->输出
    (1)Spark RDD：
        RDD：弹性分布式数据集
        RDD是整个Spark的计算基石。是分布式数据的抽象，为用户屏蔽了地层复杂的计算和映射环境。
        RDD是不可变的，如果在一个RDD上进行转换操作，则会生成一个新的RDD
        RDD是分区的，RDD里面的具体数据是分布在多台机器上的Executor里面的。堆内内存和堆外内存+磁盘
        RDD是弹性的：
            存储：Spark会根据用户的配置或者当前Spark的应用运行情况自动将RDD的数据缓存到内存或磁盘。
                  他是一个对用户不可见的封装的功能（内存与磁盘的自动切换）
            容错：当你的RDD数据被删除或者丢失的时候，可以通过血统或者检查点机制恢复数据。
            计算：计算是分层的，有应用->Job->Stage->TaskSet-Task  每一层都有对应的计算的保障与重复
                  机制。保障你的计算不会由与一些突发因素而终止。
            分片：你可以根据业务需求或者一些算子来重新调整RDD中的数据分布。   
        **  RDD的数据处理方式类似于IO流，也有装饰者设计模式
        **  RDD是不保存数据的，但是IO可以临时保存一部分数据
    
        RDD的核心属性：
            分区列表：用于执行任务时并行计算，是实现分布式计算的重要属性。
            分区计算函数：使用分区函数对每个分区进行计算
            RDD之间的依赖关系：需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系
            分区器  
            首选位置
            
        RDD的三种创建方式：
            1.可以从一个Scala集合里面创建
                sc.parallelize(seq)  把seq这个数据并行化分片到节点
                sc.makeRDD(seq)  把seq这个数据并行化分片到节点，他的实现就是parallelize
                sc.makeRDD(seq[(T,seq)])  这种方式可以指定RDD的存放位置
            2.从外部存储来创建  sc.textFile("FilePath")
            3.从另外一个RDD转换而来
        
        RDD的操作针对两种类型的RDD：
            数值RDD
            键值对RDD
        RDD的操作分为2种：转换(transformations)和行动(action)
            转换：通过操作将一个RDD转换成另外一个RDD
            行动：将一个RDD进行求值或输出
            (RDD的所有转换操作都是懒执行的，只有当行动操作出现spark才真的去运行)
            
        常用的转换算子(transformations)：
            def map[U:ClassTag](f:T=>U):RDD[U]  将函数应用于RDD的每一元素，并返回一个新的RDD
            def filter(f:T=>Boolean):RDD[T]  通过提供的产生boolean条件的表达式来返回结果为True的新的RDD
            def flatMap[U:ClassTag](f:T=>TraversableOnce[U]):RDD[U]  将函数应用于RDD中的每一项，
                                            对于每一项都产生一个集合，并将集合中的元素压扁成一个集合
                sc.makeRDD(List(List(1,2),3,List(4,5))).flatMap(
                                                            data => {
                                                            data match {
                                                                case list:List[_] => list
                                                                case dat => List(dat)
                                                            }
                                                            })
                
            def mapPartitions[U:ClassTag](f:Iterator[T]=>Iterator[U],preservesPartitioning:Boolean=false):RDD[U]
                将函数应用于RDD的每一个分区，每一个分区运行一次，函数需要能够接受Iterator类型，然后返回Iterator
                rdd.mapPartitionsWithIndex((index, iter) => {
                                            if (index == 1) {
                                                iter
                                            } else
                                                Nil.iterator
                                            })
            
            def mapPartitionsWothIndex  将函数应用于RDD中的每一个分区，每一个分区运行一次，函数能够
                    接受一个分区的索引值和一个代表分区内所有数据的Iterator类型，需要返回Iterator类型
            def sample(withReplacement:Boolean,fraction:Double,seed:Long={})  在RDD中以seed为种子
                    返回大致上有fraction比例个数据样本RDD，withReplacement表述是否采用放回式抽样 
                sc.makeRDD(List(1,2,3,4,5,6,7,8,9,10)).sample(true, 0.4).collect().mkString(",")
                
            def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)]  做两个RDD的笛卡尔积，返回对偶的RDD
            def pipe(command: String): RDD[String]  对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。
                注意，如果你是本地文件系统中，需要将脚本放置到每个节点上
            def coalesce(numPartitions: Int, shuffle: Boolean = false,
                            partitionCoalescer: Option[PartitionCoalescer] = Option.empty)
                            (implicit ord: Ordering[T] = null)
                     缩减分区数，用于大数据集过滤后，提高小数据集的执行效率,默认情况下不会将分区的数据打乱重新组合
                     可能会导致数据不均衡，导致数据倾斜
                sc.makeRDD(List(1,2,3,4,5,6),3).coalesce(2)
                sc.makeRDD(List(1,2,3,4,5,6),2).coalesce(3,true)  shuffle=true可以数据均衡，也可以实现扩大分区。
                
            def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
                    根据你传入的分区数重新通过网络分区所有数据，重型操作
                    底层使用coalesce
                    
            def union(other:RDD[T]):RDD[T]  将两个RDD中的元素合并，并返回一个新的RDD(两个RDD数据类型一致)
            
            def intersection(other:RDD[T]):RDD[T]  将两个RDD中的元素做交集，并返回一个新的RDD(两个RDD数据类型一致)
                rdd = sc.makeRDD(List(1,2,3,4)).intersection(sc.makeRDD(List(3,4,5,6)))
            
            def subtract(other: RDD[T]): RDD[T]  计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来(两个RDD数据类型一致)
                        
                        
            def distinct():RDD[T]  将当前RDD进行去重后，返回一个新的RDD
            def sortBy[K](f:(T)=>K,ascending:Boolean=true,numPartitions:Int=this.partitions.length)
            (implicit ord:Ordering[K],ctag:ClassTag[K]):RDD[T]  底层实现还是使用sortByKey，只不过使用fun生成的新key进行排序
                sc.makeRDD(List(8,5,3,6,7,2,1,4)).sortBy(t=>t._1,false)
            
                        
            def glom():RDD[Array[T]]    将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]
                sc.makeRDD(List(1,2,3,4),2).glom().map(arr => arr.max)
            
            def mapValues[U](f: V => U): RDD[(K, U)]   将函数应用于（k，v）结果中的v，返回新的RDD
            
       <k,v>def partitionBy(partitioner:Partitioner):RDD[(K,V)]  根据设置的分区器重新将RDD进行分区，返回新的RDD
       <k,v>def reduceByKey(func:(V,V)=>V):RDD[(K,V)]  根据Key值将相同Key的元组的值用func计算，返回新的RDD
            如果key只有1个，不参加两两聚合；
            支持分区内预聚合(combine)功能，可以有效减少shuffle时落盘的数据量；
            分区内和分区间计算规则相同
            
       <k,v>def aggregateByKey[U:ClassTag](zeroValue:U,partitioner:Partitioner)(seqOp:(U,V)=>U,comOp:(U,U)=>U):RDD[(K,U)]
                通过seqOp函数将每一个分区里面的数据和初始值迭代代入函数返回最终值，comOp将每一个分区
                返回的最终值根据key进行合并操作。
                [("a",1),("a",2)],[("a",3),("a",4)]
                [("a",2)],[("a",4)]
                [("a",6)]
                val rdd = sc.makeRDD(List(("a",1),("a",2),("a",3),("a",4)),2)
                rdd.aggregateByKey(0)((x,y)=>math.max(x,y),(x,y)=>x+y)
                最终的返回结果和初始值的类型保持一致
                
       <k,v>def groupByKey():RDD[(K,Iterable[V])]  将相同的Key值聚集，输出一个(K,Iterable[V])的RDD
            将相同key的数据分在一个组中，形成一个对偶元组，元组的第一个元素是key，第二个元素是相同key的value的集合
            只分组，不聚合
       
       <k,v>def combineByKey(createCombiner:V=>C,
                            mergeValue:(C,V)=>C,
                            mergeCombiners:(C,C)=>C,
                            numPartitions:Int):RDD[(K,C)]
                    根据Key分别使用CreateCombiner和mergeValue进行相同key的数据聚集，通过mergeCombiners将
                各个分区最终的结果进行聚集。
                第一个参数：相同key的第一个数据进行结构的转换，实现操作
                第二个参数：分区内计算规则
                第三个参数：分区间计算规则

       <k,v>def foldByKey(zeroValue: V,partitioner: Partitioner)(func: (V, V) => V): RDD[(K, V)]    
                aggregateByKey的简化操作，seqop和combop相同
                当aggregateByKey 分区间和分区内计算规则相同时可用 foldByKey 替代
                
       <k,v>def sortByKey(ascending:Boolean=true,numPartitions:Int=self.partitions.length):RDD[(K, V)]
                在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD
            
       <k,v>def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]    在类型为(K,V)和(K,W)的RDD上调用，
                返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。
                需要注意的是，他只会返回key在两个RDD中都存在的情况(key没匹配上就不出现在结果中)
                如果两个数据源中的key多个匹配，可能会出现笛卡尔积
                
                
       <k,v>def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Iterable[V], Iterable[W]))]
                    在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD，注意，如果
                V和W的类型相同，也不放在一块，还是单独存放.
                分组+连接
            
       <k,v>def repartitionAndSortWithinPartitions(partitioner: Partitioner): RDD[(K, V)]
                    性能要比repartition要高。在给定的partitioner内部进行排序

        reduceByKey:
            combineByKeyWithClassTag[V](
                (v:V)=> v,    // 第一个值不会参与计算
                func,         // 分区内计算规则
                func          // 分区间计算规则
            )
            
        aggregateByKey:
            combineByKeyWithClassTag[U](
                (v:V)=>cleanedFunc(createZero(),v),    // 初始值和第一个key的value的值进行的分区内数据操作
                cleanedSeqOp,                          // 分区内计算规则
                comOp,                                 // 分区间计算规则
            )
            
        foldByKey:
            combineByKeyWithClassTag[V](
                (v:V)=>cleanedFunc(createZero(),v),   // 初始值和第一个key的value的值进行的分区内数据操作
                cleanedFunc,                          // 分区内计算规则
                cleanedFunc,                          // 分区间计算规则
            )
            
        combineByKey:
            combineByKeyWithClassTag(
                createCombiner,        // 相同key的第一条数据进行的处理
                mergeValue             // 分区内数据的处理函数
                mergeCombiners         // 分区间数据的处理函数
            )
            
        行动算子(action)：
            def takeSample(withReplacement:Boolean,num:Int,seed:Long):Array[T]  抽样但是返回一个scala集合
            def reduce(f: (T, T) => T): T   通过func函数聚集RDD中的所有元素
            def collect(): Array[T]  在驱动程序中，以数组的形式返回数据集的所有元素

            def first(): T   返回RDD中的第一个元素
            def take(num: Int): Array[T]  返回RDD中的前n个元素
            def takeOrdered(num: Int)(implicit ord: Ordering[T])   返回前几个的排序
            def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T)=> U, combOp:(U, U)=>U):U     
                    aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用
                combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数
                最终返回的类型不需要和RDD中元素类型一致
                初始值会参与分区内计算，并参与分区间计算
                
            def fold(zeroValue: T)(op:(T, T) => T): T  折叠操作，aggregate的简化操作，seqop和combop一样
            def saveAsTextFile(path: String): Unit   将RDD以文本文件的方式保存到本地或者HDFS中
            def saveAsObjectFile(path: String): Unit  将RDD中的元素以序列化后对象形式保存到本地或者HDFS中。
            def count(): Long   返回RDD中的元素个数
            def countByKey(): Map[K, Long]  针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数
            def countByValue()
            
            def foreach(f: T => Unit): Unit  在数据集的每一个元素上，运行函数func进行更新


        
        
        
    (2)spark提交应用程序
        ****Spark提交应用程序
            1.进入到spark安装目录的bin，调用Spark-submit脚本
            2.在脚本后传入参数
                --class  你的应用的主类
                --master  你的应用运行的模式 Local、Local[N]、Spark://hostname:port、Mesos、Yarn-client、Yarn-cluster
                [可选]  指定 --deploy-mode 为 client模式或cluster模式
                应用jar包的位置
                应用的参数
        
        ****Spark调试
            本地调试[以单节点的方式运行整个Spark应用]：
                1、写好程序
                2、将master设置为local或者local[n]
                3、报错winutils.exe找不到，将HADOOP_HOME环境变量加入IDEA
                4、Debug调试
            远程调试[把IDEA当做Driver，保持和整个Spark集群的连接关系]
            前提：本机和spark集群在同一网段
            1、写好程序
            2、将master设置为spark集群地址[spark://hostname:port]
            3、将最终需要运行的jar包加入到setJar方法中
            4、设置本地地址到spark.driver.host这个变量中
            5、Debug调试
                
    (3)执行原理
            Spark框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务，然后将任务分发到已经分配
        资源的计算节点上，按照指定的计算模型进行数据计算，最后得到计算结果。
        
            Spark中，shuffle操作必须落盘处理，不能在内存中数据等待，会导致内存溢出。shuffle性能非常低。
        
        1.启动Yarn集群环境
        2.Spark通过申请资源创建调度节点和计算节点
        3.Spark框架根据需求将计算逻辑根据分区划分成不同的任务
        4.调度节点将任务根据计算节点状态发送到对应的计算节点进行计算
        
        Spark的任务调度分两路：一路是Stage的调度 DAGScheduler，一路是Task的调度 TaskScheduler 
        任务调度 TaskScheduler 默认FIFO调度方式，另一种是 FAIR 公平调度
        
        移动数据不如移动计算：（task的本地化调度）
        计算和数据的位置存在不同的级别，这个级别称为本地化级别
        进程本地化：数据和计算在同一个进程中
        节点本地化：数据和计算在同一个节点中
        机架本地化：数据和计算在同一个机架中
        any
        
            RDD在整个流程中主要是将逻辑进行封装，并生成Task发送给Executor节点执行计算
        
        关于读取文件(单个文件)：spark读取文件，采用Hadoop的方式读取，一行一行读；读取时以偏移量为单位，偏移量不会被重复读取
        e.g. 假如有一文件内容如下：
            /*
                1@@
                2@@
                3
            */
            一共7字节(含回车换行符),sc.textFile("datas/1.txt",3)第二个参数表示设定分区数，但 7byte/3=2······1
            多出来的1字节占每个分区的1/3=33% 超过了10%，按Hadoop分区规则加一个分区，所以一个3个分区
            0号分区：[0,3] => 1@@2 
            1号分区：[3,6] => 3 
            2号分区：[6,7] =>  
        多个文件，以文件为单位 
        
        RDD的依赖关系
        
        多个连续RDD的依赖关系，称之为血缘关系
        每个RDD都会保存血缘关系
        RDD不保存数据，为了提供容错性，需要将RDD之间的数据保存下来，一旦出现错误，可以根据血缘关系将数据源重新读取计算
        
        1、RDD的依赖关系分为窄依赖和宽依赖。
        2、窄依赖是说父RDD的每一个分区最多被一个子RDD的分区应用，也就是他的出度为1。
        3、宽依赖是说父RDD的每一个分区被多个子RDD的分区来应用，也就是他的出度大于等于2.
        4、应用在整个过程中，RDD之间形成的产生关系，就叫做血统关系，RDD在没有持久化的时候
           默认是不保存的，如果需要那么就要根据血统关系来重新计算。
        5、应用在执行过程中，是分为多个Stage来进行的，划分Stage的关键就是判断是不是存在
           宽依赖。从Action往前去推整个Stage的划分。
        
        RDD的阶段划分
        当RDD中存在shuffle依赖时，阶段会自动增加一个
        阶段的数量 = shuffle依赖的数量 + 1
        ResultStage只有一个，最后需要执行的阶段
        
        RDD任务划分
        RDD任务切分中间分为：Application、Job、Stage和Task
        Application：初始化一个SparkContext即生成一个Application
        Job：一个Action算子就会生成一个Job
        Stage：Stage等于宽依赖(ShuffleDependency)的个数加1
        Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数
        任务的数量 = 当前阶段中最后一个RDD的分区数量
        
        RDD的持久化
        不一定为了重用，数据执行较长，或数据较重要的场合也可以使用持久化操作
        检查点：
        checkpoint需要落盘，需要指定检查点保存路径
        一般保存在分布式存储系统中
        cache：将数据临时存储在内存中进行数据重用
                会在血缘关系中添加新的依赖。一旦出现问题，可以从头读取数据
        persist：将数据临时存储在磁盘文件中进行数据重用
                 涉及到磁盘IO，性能较低，但是数据安全
                 如果作业完成，临时保存的文件就会丢失
        checkpoint：将数据长久地保存在磁盘文件中进行数据重用
                    设计到磁盘IO，性能较低，但是数据安全
                    为了保证数据安全，一般情况下会单独执行作业
                    为了能够提高效率，一般情况下是和cache联合使用
                    执行过程中，会切断血缘关系，重新建立新的血缘关系
                    checkpoint等同于改变数据源
           
        
        
    (4)编程相关
        scala中类的构造参数其实是类的属性，构造参数需要进行闭包检测，其实就等同于类进行闭包检测
        序列化：
            val conf = new SparkConf().setAppName("serDemo").setMaster("")
                                      .set("spark.serializer","org.apache.spark.serializer.KryoSerializer")    //替换默认的序列化机制
                                      .registerKryoClasses(Array(classOf[DemoClass]))    //注册需要使用kryo序列化的类

    (5)累加器：
            分布式共享只写变量。
            累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一个副本，
        每个task更新这些副本的值后，传回Driver端进行merge
        少加：转换算子调用累加器，如果没有行动算子，不会执行
        多加：行动算子多次执行
        一般情况下累加器会放置在行动算子中操作
        
            自定义累加器
            1. 继承AccumulatorV2，定义泛型
                IN：
                OUT：
            2.重写方法
    (6)广播变量
            分布式共享只读变量
四、SparkSQL
    (1)特点：
            1.易整合
            2.使用相同方式连接不同的数据源
            3.兼容hive
            4.标准数据连接JDBC或ODBC
    
    (2)使用
                
        spark.sql("select * from json.`datas/users.json`").show()
                
    (3)RDD和DataFrame的转换
                
    (4)RDD、DataFrame、DataSet的对比
       共性：
          RDD、DataFrame、DataSet都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利
          三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，才进行遍历预算
          三者有许多共同的函数，如filter、排序等
          对DataFrame和DataSet进行许多操作都需要 import spark.implicits._ (在创建好SparkSession对象【变量名spark】后尽量直接导入)
          三者都会根据spark的内存情况进行自动缓存运算，即使数据量很大也不用担心数据溢出
          三者都有 partition 的概念
          DataFrame和DataSet均可使用模式匹配获取各个字段的值和类型
       区别：
          RDD：
               RDD不和spark mllib 一起使用
               RDD不支持 sparksql 操作
          DataFrame：
               DataFrame 每一行的类型固定为 Row ，每一列的值没法直接访问，只有通过解析才能获取各个字段的值
               DataFrame 和 DataSet 一般不用 spark mllib 连用
               DataFrame 和 DataSet 支持sparksql，还能注册临时表、视图，进行SQL操作
               DataFrame 和 DataSet 支持一些特别方便的保存方式，比如保存成csv，可以带上表头
          DataSet：
               DataFrame 和 DataSet 拥有完全相同的成员函数，区别只是每一行的数据类型不同
               DataFrame其实就是DataSet的一个特例 type DataFrame = DataSet[Row]
               
                
    (5)数据加载和保存
       通用的：(默认parquet格式，通过 spark.sql.sources.default 修改)
            加载：    val df: DataFrame = spark.read.[format("json").]load("datas/users.json")
            保存：    df.write.[mode("")][format("json").]save("output")
        mode:
            error     | SaveMode.ErrorIfExists -> 如果文件已存在抛出异常
            append    | SaveMode.Append        -> 如果文件已存在则追加
            overwrite | SaveMode.Overwrite     -> 如果文件已存在则覆盖
            ignore    | SaveMode.Ignore        -> 如果文件已存在则忽略
       快捷：
            加载：    val df: DataFrame = spark.read.json("datas/users.json")
            保存：    df.write.json("output")

    (6)SparkSQL连接hive
        内置hive：spark.sql("") 直接写hive SQL 语句
        外置hive：
            命令行：hive-site.xml 放到 SPARK_HOME 的conf目录下
                    core-site.xml 放到 SPARK_HOME 的conf目录下
                    hdfs-site.xml 放到 SPARK_HOME 的conf目录下
                    mysql 驱动jar 放到 SPARK_HOME 的jar 目录下
            idea： 
                1.  hive-site.xml 放到 resources 目录下
                    core-site.xml 放到 resources 目录下
                    hdfs-site.xml 放到 resources 目录下
                2.  启用hive支持
                    SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()
                3.  增加对应的依赖关系
                    mysql驱动

五、Spark Streaming
	(1)